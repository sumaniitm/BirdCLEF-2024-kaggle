{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install duckdb --no-index --find-links=file:///kaggle/input/birdclef24-duckdb-polars/kaggle/working/mysitepackages/duck_pkg\n!pip install polars --no-index --find-links=/kaggle/input/birdclef24-duckdb-polars/kaggle/working/mysitepackages/polars_pkg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport duckdb as dd\nimport polars as pl\nimport pyarrow\nimport os\nimport glob\nimport shutil\nimport zipfile\nimport matplotlib.pyplot as plt\nplt.style.use('dark_background')\nimport seaborn as sns\nimport plotly.express as px\nimport librosa\nfrom IPython.display import Audio\nimport pickle\nfrom joblib import dump, load\nfrom pathlib import Path\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to the directory containing your audio dataset\ndataset_dir = '/kaggle/input/birdclef-2024/train_audio'\n# Initialize an empty dictionary to store the mapping between audio files and labels\nlabel_mapping = {}\n# Iterate over subdirectories (classes) in the dataset directory\nfor label in os.listdir(dataset_dir):\n    label_dir = os.path.join(dataset_dir, label)\n    # Check if the item in the dataset directory is a directory\n    if os.path.isdir(label_dir):\n        # Iterate over audio files in the subdirectory (class)\n        for audio_file in os.listdir(label_dir):\n            # Add the mapping between audio file path and label to the dictionary\n            audio_file_path = os.path.join(label_dir, audio_file)\n            label_mapping[audio_file_path] = label\n            \n# label_mapping\n\n# Create a list of tuples containing the audio file paths and labels\ndata = [(audio_file_path, label) for audio_file_path, label in label_mapping.items()]\n# Create a Pandas DataFrame from the list of tuples\nannotated_data = pd.DataFrame(data, columns=['audio_file_path', 'label'])\n\nlabel_encoder = LabelEncoder()\nannotated_data['encoded_label'] = label_encoder.fit_transform(annotated_data['label'])\n\nprint(annotated_data.head(5))\nprint(annotated_data.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dd.sql(\"select label, count(distinct(audio_file_path)) as files from annotated_data group by label order by 2\").pl()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to the directory containing your audio dataset\ndataset_dir = '/kaggle/input/birdclef-2024/unlabeled_soundscapes'\n# Initialize an empty dictionary to store the mapping between audio files and labels\nlabel_mapping = {}\n\nfor audio_file in os.listdir(dataset_dir):\n    # Add the mapping between audio file path and label to the dictionary\n    audio_file_path = os.path.join(dataset_dir, audio_file)\n    label_mapping[audio_file_path] = 'unlabelled'\n            \n# label_mapping\n\n# Create a list of tuples containing the audio file paths and labels\ndata = [(audio_file_path, label) for audio_file_path, label in label_mapping.items()]\n# Create a Pandas DataFrame from the list of tuples\nunannotated_data = pd.DataFrame(data, columns=['audio_file_path', 'label'])\n\n# label_encoder = LabelEncoder()\nunannotated_data['encoded_label'] = 999\n\nprint(unannotated_data.head(5))\nprint(unannotated_data.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data = dd.sql(\"select * from annotated_data where label = 'niwpig1' union select * from unannotated_data\").pl()\ncombined_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_file_duration(full_file_path):\n    duration = librosa.get_duration(path=full_file_path, sr=32000)\n    return round(duration,2)\n\nvect_func = np.vectorize(get_file_duration)\n\ncombined_data_pd = combined_data.to_pandas()\n\ncombined_data_pd['file_duration'] = vect_func(combined_data_pd['audio_file_path'])\nprint(combined_data_pd.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_per_species_w_rnk = dd.sql(\" select *, row_number()over(partition by label, encoded_label order by file_duration desc) as rn \\\nfrom combined_data_pd where file_duration <= 240\").pl().sort(by=['encoded_label','rn'])\n\nfiles_per_species_final = files_per_species_w_rnk.filter(pl.col('rn')<=35)\n\nprint(files_per_species_final.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n#labels = []\nfeatures = []\n#feature_dict = {}\n\n# Loop through each audio file in the dataset directory\nfor i in tqdm(range(files_per_species_final.shape[0]), desc = 'Outer Loop'):\n    # labels.append(annotated_data['label'].iloc[i])\n    file_path = files_per_species_final.item(i,0)\n    # lat = files_per_species_final.item(i,3)\n    # lon = files_per_species_final.item(i,4)\n    audio, sample_rate = librosa.load(file_path, sr=32000)\n    samples_per_segment = sample_rate * 5\n    if len(audio) > 7680000:\n        total_samples = 7680000\n    else:\n        total_samples = len(audio)\n\n    for j in range(0, total_samples+160000, samples_per_segment):\n        if j + samples_per_segment <= total_samples:\n            segment = audio[j:j + samples_per_segment]\n            # chunk = cur_file+'_'+str(int((j/32000)+5))\n            # mfccs = librosa.feature.mfcc(y=segment, sr=32000, n_mfcc=40)\n            # flattened_features = (np.mean(mfccs.T, axis=0))\n            # melspec = librosa.feature.melspectrogram(y=segment, sr=32000, n_fft=500, hop_length = 50)\n            mfccs = librosa.feature.mfcc(y=segment, sr=32000, n_mfcc=40)\n            # melspec = librosa.feature.melspectrogram(y=segment, sr=sr, n_fft=n_fft, n_mels=n_mels, hop_length=hop_length, fmin=fmin, fmax=fmax)\n            # mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_fft=n_fft, n_mels=n_mels, hop_length=hop_length, fmin=fmin, fmax=fmax)\n            # flattened_melspec_features = (np.mean(melspec.T, axis=0))\n            \"\"\"flattened_mfcc_features = (np.mean(mfccs.T, axis=0))\n            flattened_features = np.append(flattened_melspec_features, flattened_mfcc_features)\"\"\"\n            # features.append(np.append(flattened_features, np.array([lat,lon])))\n            flattened_features = (np.mean(mfccs.T, axis=0))\n            #features.append(flattened_features)\n            #labels.append(files_per_species_final.item(i,2))\n            features.append({'file_path': file_path, 'label': files_per_species_final.item(i,2), 'feature_vector': flattened_features})\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df = pd.DataFrame.from_dict(features)\nprint(features_df.head(5))\nprint(features_df.tail(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_matrix = np.vstack(features_df.feature_vector.values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cluster in n_clusters:\n    print(\"starting kmeans with {0} clusters\".format(cluster))\n    kmeans = KMeans(n_clusters = cluster, init='k-means++', random_state=42)\n    kmeans.fit(feature_matrix)\n    \n    print(\"inertia = {0}\".format(kmeans.inertia_))\n    list_inertia.append(kmeans.inertia_)\n    \n    sh_score = silhouette_score(feature_matrix, kmeans.labels_)\n    print(\"silhouette score = {0}\".format(sh_score))\n    list_sh_score.append(sh_score)\n    \nmetrics_tracking['inertia'] = list_inertia\nmetrics_tracking['silhouette_score'] = list_sh_score\n\ntest_df = pd.DataFrame(metrics_tracking)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(test_df, x='n_clusters', y='silhouette_score', markers=True)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(test_df, x='n_clusters', y='inertia', markers=True)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 35, init='k-means++', random_state=42)\nkmeans.fit(feature_matrix)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df['cluster'] = kmeans.labels_\n\ndef get_cluster_centroids(cluster_id):\n    return kmeans.cluster_centers_[cluster_id]\n\nfeatures_df['centroid'] = features_df['cluster'].apply(get_cluster_centroids)\n\ndef get_ftr_w_centroid(feature, cluster_id):\n    list_of_ftr_n_cc = [feature, kmeans.cluster_centers_[cluster_id]]\n    return list_of_ftr_n_cc\n\nfeatures_df['ftr_w_centroid'] = features_df.apply(lambda x: get_ftr_w_centroid(x['feature_vector'], x['cluster']), axis=1)\n\ndef calc_cos_sim(ftr_w_centroid):\n    v = ftr_w_centroid[0]\n    w = ftr_w_centroid[1]\n    \n    cos_before_norm = np.dot(v, w) / (np.linalg.norm(v) * np.linalg.norm(w))\n    rounded_cos_sim = round(cos_before_norm, 2)\n    return rounded_cos_sim\n\nfeatures_df['cosine_similarity'] = features_df['ftr_w_centroid'].apply(calc_cos_sim)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_clusters = dd.sql(\" select distinct cluster from ( select *, row_number()over(order by files desc) as rn from ( select cluster, count(distinct(label)) as data_count, count(distinct(file_path)) as files \\\nfrom features_df group by cluster )t1 )t2 where rn <= 5\").pl().to_series().to_list()\n\nprint(list_of_clusters)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df['label'] = np.where(features_df['cluster'].isin(list_of_clusters), 119, features_df['label'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_list_of_files = dd.sql(\"select distinct file_path from features_df where label = 119 \").pl().to_series().to_list()\n\nprint(final_list_of_files)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data_pd['label'] = \\\nnp.where(combined_data_pd['audio_file_path'].isin(final_list_of_files), 'niwpig1', combined_data_pd['label'])\n\ncombined_data_pd['encoded_label'] = \\\nnp.where(combined_data_pd['audio_file_path'].isin(final_list_of_files), 119, combined_data_pd['encoded_label'])\n\ncombined_data_pd[combined_data_pd['audio_file_path'].isin(final_list_of_files)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Test to see if the same file has been put under more than one class\ndd.sql(\" select audio_file_path, count(distinct(encoded_label)) as encoded_labels \\\nfrom combined_data_pd group by audio_file_path having encoded_labels > 1 \").pl()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data_pd_niwpig1 = combined_data_pd[combined_data_pd['audio_file_path'].isin(final_list_of_files)].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data_pd_niwpig1.to_csv('combined_data_pd_niwpig1.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}